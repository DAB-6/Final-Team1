{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4062548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.56.1\n",
      "Uninstalling transformers-4.56.1:\n",
      "  Successfully uninstalled transformers-4.56.1\n",
      "Found existing installation: accelerate 1.10.1\n",
      "Uninstalling accelerate-1.10.1:\n",
      "  Successfully uninstalled accelerate-1.10.1\n",
      "Collecting transformers==4.40.2\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "Collecting accelerate==0.30.1\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (2.32.5)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from transformers==4.40.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from accelerate==0.30.1) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from accelerate==0.30.1) (2.6.0.dev20241112+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from torch>=1.10.0->accelerate==0.30.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.30.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from tqdm>=4.27->transformers==4.40.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.30.1) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from requests->transformers==4.40.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from requests->transformers==4.40.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from requests->transformers==4.40.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from requests->transformers==4.40.2) (2025.8.3)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 5.5/9.0 MB 28.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 29.4 MB/s  0:00:00\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 9.0 MB/s  0:00:00\n",
      "Installing collected packages: tokenizers, accelerate, transformers\n",
      "\n",
      "  Attempting uninstall: tokenizers\n",
      "\n",
      "    Found existing installation: tokenizers 0.22.0\n",
      "\n",
      "    Uninstalling tokenizers-0.22.0:\n",
      "\n",
      "      Successfully uninstalled tokenizers-0.22.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ---------------------------------------- 0/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [accelerate]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed accelerate-0.30.1 tokenizers-0.19.1 transformers-4.40.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\miniconda3\\envs\\torch-gpu\\Lib\\site-packages\\~-kenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers accelerate\n",
    "!pip install \"transformers==4.40.2\" \"accelerate==0.30.1\" safetensors -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454b2ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐시 삭제: C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M\n",
      "정리 완료\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "loc = \"facebook/nllb-200-distilled-600M\"\n",
    "if os.path.isdir(loc):\n",
    "    print(\"로컬폴더 발견 → 삭제:\", loc)\n",
    "    shutil.rmtree(loc, ignore_errors=True)\n",
    "\n",
    "cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"models--facebook--nllb-200-distilled-600M\"\n",
    "print(\"캐시 삭제:\", cache)\n",
    "shutil.rmtree(cache, ignore_errors=True)\n",
    "\n",
    "print(\"정리 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23c3399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐시 발견 → 삭제: C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M\n",
      "✅ 캐시/폴더 정리 완료\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "from pathlib import Path\n",
    "\n",
    "loc = \"facebook/nllb-200-distilled-600M\"\n",
    "if os.path.isdir(loc):\n",
    "    print(\"로컬폴더 발견 → 삭제:\", loc)\n",
    "    shutil.rmtree(loc, ignore_errors=True)\n",
    "\n",
    "cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\" / \"models--facebook--nllb-200-distilled-600M\"\n",
    "if cache.exists():\n",
    "    print(\"캐시 발견 → 삭제:\", cache)\n",
    "    shutil.rmtree(cache, ignore_errors=True)\n",
    "\n",
    "print(\"✅ 캐시/폴더 정리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11f53cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENV & PATH ===\n",
      "python: c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\python.exe\n",
      "torch: 2.6.0.dev20241112+cu121 cuda: 12.1 avail: True\n",
      "transformers: 4.40.2\n",
      "huggingface_hub: 0.35.0\n",
      "safetensors: 0.6.2\n",
      "\n",
      "=== CWD & FILES ===\n",
      "cwd: c:\\Users\\user\\Downloads\\mscd\\archive\n",
      "files in cwd (first 100):\n",
      "  FPS_steam_reviews_20_games.csv\n",
      "  data\n",
      "  engtranslator.ipynb\n",
      "  fast_out\n",
      "  opus-mt-mul-en\n",
      "  out\n",
      "  weighted_score_above_08.csv\n",
      "  weighted_score_above_08.csv.zip\n",
      "  가설.md\n",
      "  게임별_리뷰당시시간_긍정_부정평가.twb\n",
      "\n",
      "local_model_dir exists: False -> c:\\Users\\user\\Downloads\\mscd\\archive\\facebook\\nllb-200-distilled-600M\n",
      "cache_dir exists: True -> C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M\n",
      "Removing cache folder: C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M\n",
      "Removed cache folder.\n",
      "\n",
      "=== NETWORK CHECK ===\n",
      "resolving huggingface.co : 3.168.178.31\n",
      "HTTP status for huggingface.co/models: 200\n",
      "\n",
      "=== HfApi.model_info check ===\n",
      "model_info ok. private? False\n",
      "\n",
      "=== ATTEMPT: from_pretrained(force_download=True, use_safetensors=True, low_cpu_mem_usage=True, device_map='auto') ===\n",
      "Tokenizer loaded OK.\n",
      "LOAD FAILED. Full traceback below:\n",
      "\n",
      "=== END DIAGNOSTIC ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3321, in from_pretrained\n",
      "    resolved_archive_file, revision, is_sharded = auto_conversion(\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\safetensors_conversion.py\", line 111, in auto_conversion\n",
      "    raise e\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\safetensors_conversion.py\", line 90, in auto_conversion\n",
      "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\safetensors_conversion.py\", line 65, in get_conversion_pr_reference\n",
      "    private = api.model_info(model_id).private\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\hf_api.py\", line 2635, in model_info\n",
      "    headers = self._build_hf_headers(token=token)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\hf_api.py\", line 9573, in _build_hf_headers\n",
      "    return build_hf_headers(\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py\", line 139, in build_hf_headers\n",
      "    hf_headers.update(headers)\n",
      "ValueError: dictionary update sequence element #0 has length 1; 2 is required\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18148\\2771511314.py\", line 85, in <module>\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3417, in from_pretrained\n",
      "    raise EnvironmentError(\n",
      "OSError: Can't load the model for 'facebook/nllb-200-distilled-600M'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/nllb-200-distilled-600M' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n"
     ]
    }
   ],
   "source": [
    "# 진단+복구 셀 (Jupyter) — 복붙해서 실행하세요.\n",
    "import os,sys,traceback,shutil,socket\n",
    "from pathlib import Path\n",
    "print(\"=== ENV & PATH ===\")\n",
    "print(\"python:\", sys.executable)\n",
    "try:\n",
    "    import torch, transformers, huggingface_hub, safetensors\n",
    "    print(\"torch:\", getattr(torch,'__version__',None), \"cuda:\", getattr(torch.version,'cuda',None), \"avail:\", torch.cuda.is_available())\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "    print(\"safetensors:\", safetensors.__version__)\n",
    "except Exception as e:\n",
    "    print(\"import-version check error:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== CWD & FILES ===\")\n",
    "cwd = Path.cwd()\n",
    "print(\"cwd:\", cwd)\n",
    "print(\"files in cwd (first 100):\")\n",
    "for i,f in enumerate(sorted(os.listdir(cwd))):\n",
    "    if i<100:\n",
    "        print(\" \",f)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "model_id = \"facebook/nllb-200-distilled-600M\"\n",
    "local_model_dir = cwd / model_id\n",
    "cache_dir = Path.home()/\".cache\"/\"huggingface\"/\"hub\"/f\"models--{model_id.replace('/','--')}\"\n",
    "print(\"\\nlocal_model_dir exists:\", local_model_dir.exists(), \"->\", local_model_dir)\n",
    "print(\"cache_dir exists:\", cache_dir.exists(), \"->\", cache_dir)\n",
    "\n",
    "# 1) remove local folder if present\n",
    "if local_model_dir.exists():\n",
    "    try:\n",
    "        print(\"Removing local folder:\", local_model_dir)\n",
    "        shutil.rmtree(local_model_dir)\n",
    "        print(\"Removed local folder.\")\n",
    "    except Exception:\n",
    "        print(\"Failed to remove local folder:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 2) remove cache folder (safe)\n",
    "if cache_dir.exists():\n",
    "    try:\n",
    "        print(\"Removing cache folder:\", cache_dir)\n",
    "        shutil.rmtree(cache_dir)\n",
    "        print(\"Removed cache folder.\")\n",
    "    except Exception:\n",
    "        print(\"Failed to remove cache folder:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 3) network check to huggingface\n",
    "print(\"\\n=== NETWORK CHECK ===\")\n",
    "try:\n",
    "    # quick DNS/socket test to huggingface.co\n",
    "    host = \"huggingface.co\"\n",
    "    print(\"resolving\",host,\":\",socket.gethostbyname(host))\n",
    "    import urllib.request, urllib.error\n",
    "    try:\n",
    "        u = urllib.request.urlopen(\"https://huggingface.co/models\", timeout=15)\n",
    "        print(\"HTTP status for huggingface.co/models:\", u.status)\n",
    "    except Exception as e:\n",
    "        print(\"HTTP access check failed:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Network/DNS check failed:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4) try HfApi.model_info (reports permissions/problems)\n",
    "print(\"\\n=== HfApi.model_info check ===\")\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    info = api.model_info(model_id)\n",
    "    print(\"model_info ok. private?\", getattr(info,'private',None))\n",
    "except Exception as e:\n",
    "    print(\"HfApi.model_info failed:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 5) attempt to load using safe, with verbose try/except\n",
    "print(\"\\n=== ATTEMPT: from_pretrained(force_download=True, use_safetensors=True, low_cpu_mem_usage=True, device_map='auto') ===\")\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, force_download=True)\n",
    "    print(\"Tokenizer loaded OK.\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id,\n",
    "        use_safetensors=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        force_download=True,\n",
    "        trust_remote_code=False\n",
    "    )\n",
    "    dev = None\n",
    "    try:\n",
    "        dev = model.get_input_embeddings().weight.device\n",
    "    except Exception:\n",
    "        try:\n",
    "            dev = next(model.parameters()).device\n",
    "        except Exception:\n",
    "            dev = \"unknown\"\n",
    "    print(\"Model loaded; param device:\", dev)\n",
    "    print(\"SUCCESS: model appears loaded. Try generate next.\")\n",
    "except Exception as e:\n",
    "    print(\"LOAD FAILED. Full traceback below:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== END DIAGNOSTIC ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd64a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\python.exe\n",
      "torch: 2.6.0.dev20241112+cu121 cuda: 12.1 avail: True\n",
      "transformers: 4.40.2\n",
      "huggingface_hub: 0.35.0\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, transformers, huggingface_hub\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda, \"avail:\", torch.cuda.is_available())\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a6d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.6\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "print(huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fe57ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.6\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub; print(huggingface_hub.__version__)  # => 0.24.6 여야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ee8502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:30<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded to: C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M\\snapshots\\f8d333a098d19b4fd9a8b18f94170487ad3f821d\n",
      "✅ model device: cuda:0\n",
      "=> Hello. I'm very pleased to meet you.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "repo = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "# 1) bin 포함 모든 파일 다운로드\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=repo,\n",
    "    revision=\"main\",\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "print(\"downloaded to:\", local_dir)\n",
    "\n",
    "# 2) 로컬 경로에서 로드 (bin 허용)\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    local_dir,\n",
    "    use_safetensors=False,          # ✅ safetensors 강제 해제\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=False,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "dev = model.get_input_embeddings().weight.device\n",
    "print(\"✅ model device:\", dev)\n",
    "\n",
    "# 3) 번역 테스트 (ko→en)\n",
    "tok.src_lang = \"kor_Hang\"\n",
    "x = tok(\"안녕하세요. 반갑습니다.\", return_tensors=\"pt\").to(dev)\n",
    "bos_id = tok.convert_tokens_to_ids(\"eng_Latn\")\n",
    "y = model.generate(**x, max_new_tokens=50, forced_bos_token_id=bos_id)\n",
    "print(\"=>\", tok.batch_decode(y, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4bfe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: cuda:0\n",
      "ko→en: Hello. I'm very pleased to meet you.\n"
     ]
    }
   ],
   "source": [
    "# NLLB-200 distilled 600M, ko↔en 번역 유틸\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "REPO = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "# 1) 로컬 스냅샷(모든 파일 포함: bin) — 최초 1회만 내려받음\n",
    "local_dir = snapshot_download(REPO, revision=\"main\", local_dir_use_symlinks=False)\n",
    "\n",
    "# 2) 로컬에서 로드 (torch 2.6+ 이므로 .bin 허용)\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    local_dir,\n",
    "    use_safetensors=False,                    # <- 이 모델은 safetensors 미제공, bin 사용\n",
    "    device_map=\"auto\",                        # 자동 배치 (GPU/CPU)\n",
    "    torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# 간단 번역 함수\n",
    "def translate(text: str, src=\"kor_Hang\", tgt=\"eng_Latn\", max_new_tokens=128):\n",
    "    tok.src_lang = src\n",
    "    dev = model.get_input_embeddings().weight.device\n",
    "    x = tok(text, return_tensors=\"pt\").to(dev)\n",
    "    bos_id = tok.convert_tokens_to_ids(tgt)\n",
    "    y = model.generate(**x, max_new_tokens=max_new_tokens, forced_bos_token_id=bos_id)\n",
    "    return tok.batch_decode(y, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"✅ device:\", model.get_input_embeddings().weight.device)\n",
    "print(\"ko→en:\", translate(\"안녕하세요. 반갑습니다.\"))        # 예시\n",
    "# print(\"en→ko:\", translate(\"Nice to meet you.\", \"eng_Latn\", \"kor_Hang\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43bd969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28264\\726360570.py:17: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 498094 columns: ['recommendationid', 'appid', 'game', 'author_steamid', 'author_num_games_owned', 'author_num_reviews', 'author_playtime_forever', 'author_playtime_last_two_weeks', 'author_playtime_at_review', 'author_last_played', 'language', 'review']\n"
     ]
    }
   ],
   "source": [
    "# 언어감지 설치 (순수 파이썬이라 바로 됨)\n",
    "!pip install -q langdetect\n",
    "\n",
    "import os, sys, math, gc, pandas as pd\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ⚠️ 경로/컬럼명만 맞춰줘\n",
    "INPUT_PATH  = \"weighted_score_above_08.csv\"   # zip이면 \"weighted_score_above_08.csv.zip\"\n",
    "OUTPUT_PATH = \"weighted_score_above_08_translated.csv\"\n",
    "REVIEW_COL  = \"review\"                         # 너 파일의 텍스트 컬럼명으로 바꿔줘\n",
    "\n",
    "# CSV/ZIP 자동 판별\n",
    "if INPUT_PATH.endswith(\".zip\"):\n",
    "    df = pd.read_csv(INPUT_PATH, compression=\"zip\")\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "assert REVIEW_COL in df.columns, f\"'{REVIEW_COL}' 컬럼을 찾지 못했어. 실제 컬럼명으로 REVIEW_COL을 바꿔줘.\"\n",
    "print(\"rows:\", len(df), \"columns:\", list(df.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c881a44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 498094\n",
      "columns: ['recommendationid', 'appid', 'game', 'author_steamid', 'author_num_games_owned', 'author_num_reviews', 'author_playtime_forever', 'author_playtime_last_two_weeks', 'author_playtime_at_review', 'author_last_played', 'language', 'review', 'timestamp_created', 'timestamp_updated', 'voted_up']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INPUT_PATH  = \"weighted_score_above_08.csv.zip\"   # zip 파일이라면 그대로 둬도 됨\n",
    "OUTPUT_PATH = \"weighted_score_above_08_translated.csv\"\n",
    "\n",
    "# CSV/ZIP 자동 판별\n",
    "if INPUT_PATH.endswith(\".zip\"):\n",
    "    df = pd.read_csv(INPUT_PATH, compression=\"zip\", low_memory=False)\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_PATH, low_memory=False)\n",
    "\n",
    "print(\"rows:\", len(df))\n",
    "print(\"columns:\", df.columns.tolist()[:15])  # 앞부분만 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1fea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ device: cuda:0\n",
      "➡ zho_Hans → eng_Latn : 160111 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "zho_Hans: 100%|██████████| 5004/5004 [6:58:55<00:00,  5.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ rus_Cyrl → eng_Latn : 50557 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rus_Cyrl: 100%|██████████| 1580/1580 [2:15:49<00:00,  5.16s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ tur_Latn → eng_Latn : 13848 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tur_Latn: 100%|██████████| 433/433 [36:06<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ spa_Latn → eng_Latn : 17458 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spa_Latn: 100%|██████████| 546/546 [50:41<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ pol_Latn → eng_Latn : 4399 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pol_Latn: 100%|██████████| 138/138 [14:32<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ por_Latn → eng_Latn : 15301 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "por_Latn: 100%|██████████| 479/479 [42:58<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ ukr_Cyrl → eng_Latn : 2715 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ukr_Cyrl: 100%|██████████| 85/85 [07:48<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ zho_Hant → eng_Latn : 5458 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "zho_Hant: 100%|██████████| 171/171 [15:28<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ deu_Latn → eng_Latn : 6759 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deu_Latn: 100%|██████████| 212/212 [19:14<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ fin_Latn → eng_Latn : 34 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fin_Latn: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ hun_Latn → eng_Latn : 419 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hun_Latn: 100%|██████████| 14/14 [01:12<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ tha_Thai → eng_Latn : 1007 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tha_Thai: 100%|██████████| 32/32 [02:55<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ jpn_Jpan → eng_Latn : 7339 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "jpn_Jpan: 100%|██████████| 230/230 [20:17<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ swe_Latn → eng_Latn : 16 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "swe_Latn: 100%|██████████| 1/1 [00:04<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ fra_Latn → eng_Latn : 5285 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fra_Latn: 100%|██████████| 166/166 [14:29<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ ita_Latn → eng_Latn : 1346 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ita_Latn: 100%|██████████| 43/43 [03:36<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ ces_Latn → eng_Latn : 824 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ces_Latn: 100%|██████████| 26/26 [02:09<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ ron_Latn → eng_Latn : 38 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ron_Latn: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ vie_Latn → eng_Latn : 32 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vie_Latn: 100%|██████████| 1/1 [00:04<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ ell_Grek → eng_Latn : 8 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ell_Grek: 100%|██████████| 1/1 [00:03<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ bul_Cyrl → eng_Latn : 14 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bul_Cyrl: 100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ nob_Latn → eng_Latn : 13 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nob_Latn: 100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ nld_Latn → eng_Latn : 18 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nld_Latn: 100%|██████████| 1/1 [00:04<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ dan_Latn → eng_Latn : 9 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dan_Latn: 100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved -> weighted_score_above_08_translated.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Steam language → NLLB 코드 매핑 + 번역 파이프라인 ---\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "INPUT_PATH  = \"weighted_score_above_08.csv.zip\"   # 입력 파일\n",
    "OUTPUT_PATH = \"weighted_score_above_08_translated.csv\"\n",
    "TEXT_COL    = \"review\"\n",
    "LANG_COL    = \"language\"\n",
    "\n",
    "# 0) 데이터 로드\n",
    "df = pd.read_csv(INPUT_PATH, compression=\"zip\" if INPUT_PATH.endswith(\".zip\") else None, low_memory=False)\n",
    "assert TEXT_COL in df.columns and LANG_COL in df.columns\n",
    "\n",
    "# 1) Steam 언어명 → NLLB 언어코드 매핑\n",
    "STEAM2NLLB = {\n",
    "    # 기본\n",
    "    \"english\":\"eng_Latn\", \"korean\":\"kor_Hang\", \"japanese\":\"jpn_Jpan\",\n",
    "    \"schinese\":\"zho_Hans\", \"tchinese\":\"zho_Hant\",\n",
    "    \"french\":\"fra_Latn\", \"german\":\"deu_Latn\",\n",
    "    \"spanish\":\"spa_Latn\", \"spanish - spain\":\"spa_Latn\",\n",
    "    \"latam\":\"spa_Latn\", \"spanish - latin america\":\"spa_Latn\",\n",
    "    \"portuguese\":\"por_Latn\", \"brazilian\":\"por_Latn\",\n",
    "    \"portuguese - brazil\":\"por_Latn\", \"portuguese - portugal\":\"por_Latn\",\n",
    "    \"italian\":\"ita_Latn\", \"russian\":\"rus_Cyrl\", \"turkish\":\"tur_Latn\",\n",
    "    \"arabic\":\"arb_Arab\", \"thai\":\"tha_Thai\", \"vietnamese\":\"vie_Latn\", \"indonesian\":\"ind_Latn\",\n",
    "    \"polish\":\"pol_Latn\", \"dutch\":\"nld_Latn\", \"swedish\":\"swe_Latn\", \"finnish\":\"fin_Latn\",\n",
    "    \"norwegian\":\"nob_Latn\", \"danish\":\"dan_Latn\",\n",
    "    \"czech\":\"ces_Latn\", \"hungarian\":\"hun_Latn\", \"romanian\":\"ron_Latn\",\n",
    "    \"greek\":\"ell_Grek\", \"ukrainian\":\"ukr_Cyrl\",\n",
    "    \"bulgarian\":\"bul_Cyrl\", \"hebrew\":\"heb_Hebr\",\n",
    "    \"croatian\":\"hrv_Latn\", \"serbian\":\"srp_Cyrl\", \"slovak\":\"slk_Latn\",\n",
    "    \"malay\":\"zsm_Latn\", \"filipino\":\"tgl_Latn\", \"tagalog\":\"tgl_Latn\",\n",
    "}\n",
    "\n",
    "def steam_to_nllb(s):\n",
    "    if not isinstance(s, str): return None\n",
    "    key = s.strip().lower()\n",
    "    return STEAM2NLLB.get(key)\n",
    "\n",
    "df[\"_nllb_src\"] = df[LANG_COL].map(steam_to_nllb)\n",
    "\n",
    "# 2) NLLB 모델/토크나이저 준비(이미 메모리에 있으면 재사용)\n",
    "try:\n",
    "    tok, model\n",
    "    DEV = model.get_input_embeddings().weight.device\n",
    "except NameError:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    REPO = \"facebook/nllb-200-distilled-600M\"\n",
    "    local_dir = snapshot_download(REPO, revision=\"main\", local_dir_use_symlinks=False)\n",
    "    tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        local_dir,\n",
    "        use_safetensors=False,        # 이 모델은 bin만 제공\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    DEV = model.get_input_embeddings().weight.device\n",
    "print(\"✅ device:\", DEV)\n",
    "\n",
    "def translate_batch(texts, src_nllb, tgt_nllb=\"eng_Latn\", max_new_tokens=256):\n",
    "    tok.src_lang = src_nllb\n",
    "    x = tok(texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEV)\n",
    "    bos_id = tok.convert_tokens_to_ids(tgt_nllb)\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(**x, max_new_tokens=max_new_tokens, forced_bos_token_id=bos_id)\n",
    "    return tok.batch_decode(y, skip_special_tokens=True)\n",
    "\n",
    "# 3) 한국어/영어는 패스, 나머지만 영어로 번역\n",
    "PASS = {\"eng_Latn\", \"kor_Hang\"}\n",
    "df[\"translated_en\"] = df[TEXT_COL].astype(str)\n",
    "\n",
    "to_idx = [i for i,(txt,src) in enumerate(zip(df[TEXT_COL].astype(str), df[\"_nllb_src\"]))\n",
    "          if (txt.strip()!=\"\") and (src is not None) and (src not in PASS)]\n",
    "\n",
    "# 언어별로 묶어서 배치 번역\n",
    "BATCH = 32\n",
    "bucket = defaultdict(list)\n",
    "for i in to_idx:\n",
    "    bucket[df.loc[i,\"_nllb_src\"]].append(i)\n",
    "\n",
    "for src, idxs in bucket.items():\n",
    "    print(f\"➡ {src} → eng_Latn : {len(idxs)} rows\")\n",
    "    for s in tqdm(range(0, len(idxs), BATCH), desc=f\"{src}\"):\n",
    "        sub = idxs[s:s+BATCH]\n",
    "        texts = df.loc[sub, TEXT_COL].astype(str).tolist()\n",
    "        try:\n",
    "            outs = translate_batch(texts, src)\n",
    "        except Exception:\n",
    "            # 실패 시 개별 처리(최소 손실)\n",
    "            outs=[]\n",
    "            for t in texts:\n",
    "                try: outs.extend(translate_batch([t], src))\n",
    "                except: outs.append(t)\n",
    "        df.loc[sub, \"translated_en\"] = outs\n",
    "\n",
    "# 4) 저장\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"✅ saved ->\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a874c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 40.1 MB/s  0:00:00\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   ---------------------------------------- 3/3 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.0 joblib-1.5.2 nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814e7057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 긍정 키워드 Top 30\n",
      "[('game', 836556), ('like', 161863), ('get', 145656), ('play', 142644), ('good', 137488), ('one', 132706), ('time', 119048), ('first', 104507), ('need', 97056), ('games', 95532), ('story', 85389), ('gameplay', 83961), ('also', 83723), ('really', 83642), ('going', 81451), ('new', 76886), ('want', 74233), ('even', 74049), ('people', 68408), ('make', 66346), ('much', 65666), ('playing', 65366), ('chinese', 64995), ('see', 62029), ('lot', 61954), ('would', 60443), ('well', 59097), ('fun', 58145), ('world', 57527), ('still', 57097)]\n",
      "\n",
      "✅ 부정 키워드 Top 30\n",
      "[('game', 255186), ('like', 49599), ('get', 47810), ('one', 37796), ('play', 37147), ('even', 36011), ('time', 35306), ('good', 27956), ('really', 27625), ('new', 25542), ('games', 23691), ('going', 23515), ('would', 23449), ('first', 23356), ('make', 22453), ('want', 21789), ('much', 20874), ('people', 20252), ('also', 20118), ('still', 19496), ('way', 18380), ('players', 18137), ('money', 18004), ('playing', 17257), ('buy', 17254), ('see', 16646), ('lot', 16479), ('gameplay', 16072), ('need', 15698), ('player', 15183)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"weighted_score_above_08_translated.zip\", compression=\"zip\", low_memory=False)\n",
    "\n",
    "# 영어 리뷰 텍스트\n",
    "reviews = df[\"translated_en\"].astype(str)\n",
    "\n",
    "# 긍/부정 라벨 (여기선 voted_up 컬럼이 있다고 가정)\n",
    "pos_reviews = df.loc[df[\"voted_up\"]==True, \"translated_en\"].dropna().tolist()\n",
    "neg_reviews = df.loc[df[\"voted_up\"]==False, \"translated_en\"].dropna().tolist()\n",
    "\n",
    "# 토큰화 + stopword 제거 함수\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^a-z ]\",\" \", text.lower())\n",
    "    return [w for w in text.split() if w not in stop and len(w)>2]\n",
    "\n",
    "# 긍정/부정별 단어 카운트\n",
    "pos_words = Counter()\n",
    "for r in pos_reviews:\n",
    "    pos_words.update(tokenize(r))\n",
    "\n",
    "neg_words = Counter()\n",
    "for r in neg_reviews:\n",
    "    neg_words.update(tokenize(r))\n",
    "\n",
    "# 상위 30개 키워드 뽑기\n",
    "print(\"✅ 긍정 키워드 Top 30\")\n",
    "print(pos_words.most_common(30))\n",
    "print(\"\\n✅ 부정 키워드 Top 30\")\n",
    "print(neg_words.most_common(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865e869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done -> weighted_score_above_08_translated_patched.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# 입력 파일들\n",
    "SRC = \"weighted_score_above_08_translated.zip\"   # 너의 원본 (zip 또는 csv)\n",
    "PATCH = \"weighted_score_above_08_translated_patch_fixed_rows.csv\"\n",
    "OUT = \"weighted_score_above_08_translated_patched.zip\"  # 결과를 zip으로 저장\n",
    "\n",
    "# 패치 로드\n",
    "p = pd.read_csv(PATCH, dtype=str, low_memory=False)\n",
    "has_id = \"recommendationid\" in p.columns\n",
    "\n",
    "# 1차 매핑: recommendationid -> fixed\n",
    "id_map = {}\n",
    "if has_id:\n",
    "    id_map = dict(p[[\"recommendationid\",\"translated_en_fixed\"]].dropna().values)\n",
    "\n",
    "# 2차 매핑(백업): (language, review) -> fixed  (id가 없거나 일부 누락 대비)\n",
    "key_map = {}\n",
    "if \"language\" in p.columns and \"review\" in p.columns:\n",
    "    key_map = dict(p[[\"language\",\"review\",\"translated_en_fixed\"]]\n",
    "                   .dropna().assign(language=lambda x: x[\"language\"].str.strip().str.lower())\n",
    "                   .set_index([\"language\",\"review\"])[\"translated_en_fixed\"].to_dict())\n",
    "\n",
    "def apply_patch_chunk(df):\n",
    "    # translated_en만 갱신\n",
    "    if \"translated_en\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # id 기준 교체\n",
    "    if has_id and \"recommendationid\" in df.columns:\n",
    "        m = df[\"recommendationid\"].astype(str).map(id_map)\n",
    "        df[\"translated_en\"] = m.combine_first(df[\"translated_en\"])\n",
    "\n",
    "    # (language, review) 기준 보조 교체\n",
    "    if key_map and {\"language\",\"review\"}.issubset(df.columns):\n",
    "        lang = df[\"language\"].astype(str).str.strip().str.lower()\n",
    "        key = list(zip(lang, df[\"review\"].astype(str)))\n",
    "        m2 = pd.Series([key_map.get(k) for k in key], index=df.index, dtype=\"object\")\n",
    "        df[\"translated_en\"] = m2.combine_first(df[\"translated_en\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "# 청크 처리 + 저장\n",
    "reader = pd.read_csv(\n",
    "    SRC,\n",
    "    compression=\"zip\" if str(SRC).endswith(\".zip\") else None,\n",
    "    low_memory=False,\n",
    "    chunksize=120_000,\n",
    "    dtype=str,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "first = True\n",
    "for chunk in reader:\n",
    "    chunk = apply_patch_chunk(chunk)\n",
    "    chunk.to_csv(\n",
    "         \"tmp_patched.csv\",\n",
    "         mode=\"w\" if first else \"a\",\n",
    "         index=False,\n",
    "         header=first,\n",
    "         encoding=\"utf-8-sig\",\n",
    "          quoting=csv.QUOTE_ALL,\n",
    "          escapechar=\"\\\\\",\n",
    "          lineterminator=\"\\n\",   # ✅ 여기! line_terminator(X) → lineterminator(O)\n",
    "        )\n",
    "    first = False\n",
    "\n",
    "# zip으로 감싸기 (엑셀 열 일 있으면 csv 그대로 써도 됨)\n",
    "import zipfile, os\n",
    "with zipfile.ZipFile(OUT, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(\"tmp_patched.csv\", arcname=\"weighted_score_above_08_translated_patched.csv\")\n",
    "os.remove(\"tmp_patched.csv\")\n",
    "\n",
    "print(\"done ->\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a19e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22944\\2000052898.py:4: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  rate = df[\"translated_en\"].astype(str).str.contains(moji).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mojibake rate: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "import re, pandas as pd\n",
    "moji = re.compile(r\"(Ã.|Â.|â..|ï..|ðŸ|鈥|�)\")\n",
    "df = pd.read_csv(\"weighted_score_above_08_translated_patched.zip\", compression=\"zip\", low_memory=False, dtype=str, encoding=\"utf-8\")\n",
    "rate = df[\"translated_en\"].astype(str).str.contains(moji).mean()\n",
    "print(\"mojibake rate:\", round(rate*100, 5), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53a710ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22944\\25629504.py:5: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"weighted_score_above_08_translated_patched.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 원본 CSV 로드 (경로 수정)\n",
    "df = pd.read_csv(\"weighted_score_above_08_translated_patched.csv\")\n",
    "\n",
    "# 정리 함수 정의\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # 1) '�' 같은 깨진 문자 제거\n",
    "    text = text.replace(\"�\", \"\")\n",
    "    # 2) 라틴계 인코딩 잔재 (Ã, Â 등) 제거\n",
    "    text = re.sub(r\"[ÃÂÏÐ]\", \"\", text)\n",
    "    # 3) 필요하다면 제어문자 제거\n",
    "    text = re.sub(r\"[\\x00-\\x1F]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# translated_en 컬럼 정리\n",
    "df[\"translated_en\"] = df[\"translated_en\"].astype(str).apply(clean_text)\n",
    "\n",
    "# 새 파일 저장\n",
    "df.to_csv(\"weighted_score_above_08_translated_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac88c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 행 수: 478708\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"weighted_score_above_08_translated_patched.csv\", low_memory=False)\n",
    "\n",
    "# 정규식 패턴\n",
    "BROKEN_RE = re.compile(r\"[\\ufffd\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\")\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w.\\-+]+@[\\w.\\-]+\\.\\w+\\b\")\n",
    "MENTION_RE = re.compile(r\"[@#][\\w\\-_.]+\")\n",
    "REPEAT_CHAR_RE = re.compile(r\"(.)\\1{4,}\")      \n",
    "NONWORD_STREAK_RE = re.compile(r\"[^\\w\\s\\u1100-\\u11FF\\u3130-\\u318F\\uAC00-\\uD7A3]{6,}\")\n",
    "MULTI_SYMBOL_LINE_RE = re.compile(r\"^[^\\w\\u1100-\\u11FF\\u3130-\\u318F\\uAC00-\\uD7A3]{8,}$\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "\n",
    "    # 깨진 문자 / 제어문자\n",
    "    s = BROKEN_RE.sub(\"\", s)\n",
    "\n",
    "    # URL / 이메일 / 멘션 제거\n",
    "    s = URL_RE.sub(\"\", s)\n",
    "    s = EMAIL_RE.sub(\"\", s)\n",
    "    s = MENTION_RE.sub(\"\", s)\n",
    "\n",
    "    # 공백 정리\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s).strip()\n",
    "\n",
    "    # 잡음 판정\n",
    "    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]\n",
    "    noisy = 0\n",
    "    for ln in lines:\n",
    "        if MULTI_SYMBOL_LINE_RE.match(ln):\n",
    "            noisy += 1\n",
    "        elif NONWORD_STREAK_RE.search(ln) or REPEAT_CHAR_RE.search(ln):\n",
    "            noisy += 1\n",
    "        elif len(ln) >= 40:\n",
    "            letters = sum(ch.isalnum() or ('\\uAC00' <= ch <= '\\uD7A3') for ch in ln)\n",
    "            if letters / max(len(ln), 1) < 0.25:\n",
    "                noisy += 1\n",
    "    if noisy >= max(1, int(0.4 * len(lines))):\n",
    "        return \"\"  # 노이즈 블록은 빈칸 처리\n",
    "\n",
    "    return s\n",
    "\n",
    "# translated_en 정리\n",
    "df[\"translated_en\"] = df[\"translated_en\"].apply(clean_text)\n",
    "\n",
    "# ===== 행 삭제: translated_en 이 빈칸/NaN 인 행 drop =====\n",
    "df = df[df[\"translated_en\"].notna() & (df[\"translated_en\"].str.strip() != \"\")]\n",
    "\n",
    "# 저장\n",
    "df.to_csv(\"weighted_score_above_08_translated_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"최종 행 수:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383647a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model/tokenizer...\n",
      "Device: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The 'low_memory' option is not supported with the 'python' engine",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 147\u001b[0m\n\u001b[0;32m    144\u001b[0m total \u001b[38;5;241m=\u001b[39m bad_before \u001b[38;5;241m=\u001b[39m retranslated \u001b[38;5;241m=\u001b[39m removed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# ─ Fix: 일부 pandas 버전은 TextFileReader가 context manager 미지원\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHUNKSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENGINE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk_idx, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reader, \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1607\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;66;03m# miscellanea\u001b[39;00m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1607\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_options_with_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1608\u001b[0m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunksize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1660\u001b[0m, in \u001b[0;36mTextFileReader._get_options_with_defaults\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1660\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1661\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(argname)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m option is not supported with the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1662\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(engine)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m engine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1663\u001b[0m             )\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1665\u001b[0m     value \u001b[38;5;241m=\u001b[39m default\n",
      "\u001b[1;31mValueError\u001b[0m: The 'low_memory' option is not supported with the 'python' engine"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ========================\n",
    "# 경로/설정 (경로만 수정)\n",
    "# ========================\n",
    "SRC = Path(r\"C:\\YOUR\\PATH\\weighted_score_above_08_translated_clean.csv\")   # 입력 CSV\n",
    "DST = Path(r\"C:\\YOUR\\PATH\\weighted_score_above_08_translated_clean_final.csv\")  # 결과 CSV\n",
    "REMOVED = Path(r\"C:\\YOUR\\PATH\\removed_rows_after_retranslate.csv\")              # 제거 목록 CSV\n",
    "\n",
    "CHUNKSIZE = 100_000\n",
    "BATCH_SIZE = 8\n",
    "MAX_LEN = 512\n",
    "ENGINE = \"python\"      # 문제 없으면 기본값으로 바꿔도 됨\n",
    "ENCODING = \"utf-8-sig\" # 저장시 Excel 호환\n",
    "\n",
    "# ========================\n",
    "# “영어만 허용” 규칙 (ASCII만 허용)\n",
    "# ========================\n",
    "NON_ASCII = re.compile(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E]\")\n",
    "BROKEN_RE = re.compile(r\"[\\ufffd\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\")  # � 및 제어문자\n",
    "\n",
    "def is_english_only(s: str) -> bool:\n",
    "    return (not s) or (NON_ASCII.search(s) is None)\n",
    "\n",
    "def clean_basic(s: str) -> str:\n",
    "    s = BROKEN_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ========================\n",
    "# 스크립트 기반 소스언어 추정\n",
    "# ========================\n",
    "SCRIPT_MAP = [\n",
    "    (re.compile(r\"[\\u3040-\\u309F\\u30A0-\\u30FF\\u31F0-\\u31FF]\"), \"jpn_Jpan\"),\n",
    "    (re.compile(r\"[\\u0400-\\u04FF]\"), \"rus_Cyrl\"),\n",
    "    (re.compile(r\"[\\u0600-\\u06FF]\"), \"arb_Arab\"),\n",
    "    (re.compile(r\"[\\u0900-\\u097F]\"), \"hin_Deva\"),\n",
    "    (re.compile(r\"[\\u0370-\\u03FF]\"), \"ell_Grek\"),\n",
    "    (re.compile(r\"[\\u0590-\\u05FF]\"), \"heb_Hebr\"),\n",
    "    (re.compile(r\"[\\u0E00-\\u0E7F]\"), \"tha_Thai\"),\n",
    "    (re.compile(r\"[\\u3400-\\u9FFF\\uF900-\\uFAFF]\"), \"zho_Hans\"),\n",
    "    (re.compile(r\"[\\u1100-\\u11FF\\u3130-\\u318F\\uAC00-\\uD7A3]\"), \"kor_Hang\"),\n",
    "]\n",
    "def guess_src_lang_one(s: str) -> Optional[str]:\n",
    "    for pat, code in SCRIPT_MAP:\n",
    "        if pat.search(s):\n",
    "            return code\n",
    "    return None\n",
    "\n",
    "# ========================\n",
    "# NLLB 모델 로딩 (예외 안전)\n",
    "# ========================\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "print(\"Loading model/tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ─ Fix: env별 tokenizer 차이 보정\n",
    "ENG_ID = None\n",
    "if hasattr(tokenizer, \"lang_code_to_id\") and isinstance(tokenizer.lang_code_to_id, dict):\n",
    "    ENG_ID = tokenizer.lang_code_to_id.get(\"eng_Latn\")\n",
    "if ENG_ID is None:\n",
    "    try:\n",
    "        ENG_ID = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "    except Exception:\n",
    "        pass\n",
    "if ENG_ID is None:\n",
    "    raise RuntimeError(\"영어 토큰 ID(eng_Latn)를 찾을 수 없습니다. transformers 최신 버전으로 업데이트 해보세요.\")\n",
    "\n",
    "def set_src_lang_safe(lang_code: str):\n",
    "    \"\"\"토크나이저 구현마다 src_lang 유무가 달라서 안전 세팅\"\"\"\n",
    "    if hasattr(tokenizer, \"src_lang\"):\n",
    "        try:\n",
    "            tokenizer.src_lang = lang_code\n",
    "        except Exception:\n",
    "            pass  # 없는 구현도 있음\n",
    "\n",
    "# ========================\n",
    "# 번역 유틸 (그룹/배치)\n",
    "# ========================\n",
    "def batch_translate_group(texts: List[str], src_lang: Optional[str]) -> List[str]:\n",
    "    outs: List[str] = []\n",
    "    effective_src = src_lang or \"eng_Latn\"  # 추정 실패 시 영어로 간주\n",
    "    set_src_lang_safe(effective_src)\n",
    "\n",
    "    start = 0\n",
    "    while start < len(texts):\n",
    "        batch = texts[start:start+BATCH_SIZE]\n",
    "        start += BATCH_SIZE\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN).to(device)\n",
    "        try:\n",
    "            gen = model.generate(\n",
    "                **enc,\n",
    "                forced_bos_token_id=ENG_ID,\n",
    "                max_length=MAX_LEN\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # ─ Fix: OOM 시 배치 줄여 재시도\n",
    "            torch.cuda.empty_cache()\n",
    "            sub_outs = []\n",
    "            for t in batch:\n",
    "                enc1 = tokenizer([t], return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN).to(device)\n",
    "                gen1 = model.generate(**enc1, forced_bos_token_id=ENG_ID, max_length=MAX_LEN)\n",
    "                sub_outs.append(tokenizer.decode(gen1[0], skip_special_tokens=True))\n",
    "            outs.extend([clean_basic(x) for x in sub_outs])\n",
    "            continue\n",
    "\n",
    "        out = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outs.extend([clean_basic(x) for x in out])\n",
    "    return outs\n",
    "\n",
    "def retranslate_to_english(texts: List[str]) -> List[str]:\n",
    "    groups: Dict[str, List[int]] = {}\n",
    "    for i, t in enumerate(texts):\n",
    "        lang = guess_src_lang_one(t) or \"eng_Latn\"\n",
    "        groups.setdefault(lang, []).append(i)\n",
    "\n",
    "    results = [\"\"] * len(texts)\n",
    "    for lang, idxs in groups.items():\n",
    "        batch = [texts[i] for i in idxs]\n",
    "        try:\n",
    "            translated = batch_translate_group(batch, src_lang=lang)\n",
    "        except Exception:\n",
    "            translated = batch  # 번역 실패 시 원문 유지 → 이후 필터에서 제거될 수 있음\n",
    "        for k, i in enumerate(idxs):\n",
    "            results[i] = translated[k]\n",
    "    return results\n",
    "\n",
    "# ========================\n",
    "# 메인 루프 (재번역→검증→삭제)\n",
    "#  - translated_en에 비ASCII(=비영어) 1자라도 있으면 재번역\n",
    "#  - 재번역 후에도 비ASCII or 빈문자열이면 행 삭제\n",
    "# ========================\n",
    "first_out, first_rm = True, True\n",
    "total = bad_before = retranslated = removed = 0\n",
    "\n",
    "# ─ Fix: 일부 pandas 버전은 TextFileReader가 context manager 미지원\n",
    "reader = pd.read_csv(SRC, chunksize=CHUNKSIZE, low_memory=False, engine=ENGINE)\n",
    "try:\n",
    "    for chunk_idx, chunk in enumerate(reader, 1):\n",
    "        if \"translated_en\" not in chunk.columns:\n",
    "            raise KeyError(\"translated_en 컬럼이 없습니다.\")\n",
    "\n",
    "        texts = chunk[\"translated_en\"].astype(\"string\")\n",
    "        total += len(texts)\n",
    "\n",
    "        needs_fix_mask = texts.fillna(\"\").apply(lambda x: not is_english_only(str(x)))\n",
    "        need_idxs = texts.index[needs_fix_mask]\n",
    "        bad_before += int(needs_fix_mask.sum())\n",
    "\n",
    "        if len(need_idxs) > 0:\n",
    "            to_fix = texts.loc[need_idxs].fillna(\"\").astype(str).tolist()\n",
    "            fixed = retranslate_to_english(to_fix)\n",
    "            retranslated += len(fixed)\n",
    "            chunk.loc[need_idxs, \"translated_en\"] = fixed\n",
    "\n",
    "        final_texts = chunk[\"translated_en\"].fillna(\"\").astype(str)\n",
    "        drop_mask = final_texts.apply(lambda x: (x.strip() == \"\") or (not is_english_only(x)))\n",
    "        removed += int(drop_mask.sum())\n",
    "\n",
    "        kept = chunk.loc[~drop_mask].copy()\n",
    "        kept.to_csv(\n",
    "            DST,\n",
    "            mode=\"w\" if first_out else \"a\",\n",
    "            header=first_out,\n",
    "            index=False,\n",
    "            encoding=ENCODING\n",
    "        )\n",
    "        first_out = False\n",
    "\n",
    "        if drop_mask.any():\n",
    "            chunk.loc[drop_mask].to_csv(\n",
    "                REMOVED,\n",
    "                mode=\"w\" if first_rm else \"a\",\n",
    "                header=first_rm,\n",
    "                index=False,\n",
    "                encoding=ENCODING\n",
    "            )\n",
    "            first_rm = False\n",
    "\n",
    "        print(f\"[Chunk {chunk_idx}] in={len(chunk):6d}  need_fix={int(needs_fix_mask.sum()):5d}  \"\n",
    "              f\"retranslated={len(need_idxs):5d}  removed_now={int(drop_mask.sum()):5d}\")\n",
    "finally:\n",
    "    try:\n",
    "        reader.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total rows processed : {total}\")\n",
    "print(f\"Rows needed fix      : {bad_before}\")\n",
    "print(f\"Rows retranslated    : {retranslated}\")\n",
    "print(f\"Rows removed (final) : {removed}\")\n",
    "print(f\"Output file          : {DST}\")\n",
    "print(f\"Removed log          : {REMOVED}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
